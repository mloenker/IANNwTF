{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_txt\n",
    "import io, datetime, tqdm, re\n",
    "import sentencepiece as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first book of moses called genesis\n",
      "\n",
      "\n",
      "in the beginning god created the heaven and the earth.\n",
      "\n",
      "and the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n"
     ]
    }
   ],
   "source": [
    "with open(\"bible.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"['\\-!:%$\\\"]\", \"\", text)\n",
    "    text = re.sub(\"[0-9]\", \"\", text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n ', '\\n', text)\n",
    "    text = text[:100000] #smaller text for faster training\n",
    "    print(text[:182])\n",
    "\n",
    "with open(\"bible2.txt\", \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input='bible2.txt', model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=1500\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
    "\n",
    "tokenizer = tf_txt.SentencepieceTokenizer(\n",
    "    model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
    "    add_bos=False, add_eos=False, return_nbest=False, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   5  238  186  959    6  857  898   77 1338   18  910   11    5  774\n",
      "   30  287    5  141    3    5   38    7    3    5   38   21  497   27\n",
      "  402    4    3 1335   53    9    3  550   21   48    5  143    6    5\n",
      "  552    7    3    5  219  728  365    6   30  277  523   48    5  143\n",
      "    6    5  128    7    3   30   16    4   87   65   23  251    3   65\n",
      "   21  251    7    3   30  149    5  251    4   15   24   21  146    3\n",
      "   30  388    5  251   43    5  550    7    3   30   77    5  251   85\n",
      "    4    3], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data, length):\n",
    "    data = tf_txt.sliding_window(data, width=length+1)\n",
    "    data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    data = data.map(lambda x: (x[:-1], x[1:])) # create input and target\n",
    "    data = data.shuffle(1000)\n",
    "    data = data.batch(32)\n",
    "    data = data.prefetch(20)\n",
    "    return data\n",
    "\n",
    "dataset = preprocessing(tokens, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, length):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.position = tf.keras.layers.Embedding(length, embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[-1]\n",
    "        positions = tf.range(0, length)\n",
    "        positions = self.position(positions)\n",
    "        x = self.embedding(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.dense_relu = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim, activation=\"relu\")\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attention_output = self.attention(inputs, inputs, use_causal_mask=True)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        out2 = self.dense_relu(out1)\n",
    "        out2 = self.dense(out2)\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        return self.layernorm2(out1 + out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, vocab_size, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim, 100)\n",
    "        self.transformer_blocks = [TransformerBlock(embedding_dim, num_heads, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.transformer_blocks[i](x, training)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.layernorm(x)\n",
    "        return self.dense(x)\n",
    "    \n",
    "    def generate_text(self, text, length, top_k):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        for _ in range(length):\n",
    "            logits = tf.expand_dims(tokens, 0)\n",
    "            logits = self(logits, training=False)\n",
    "            logits = tf.math.top_k(logits, k=top_k, sorted=True)\n",
    "            sample = tf.random.uniform(shape=(), minval=0, maxval=top_k, dtype=tf.int32)\n",
    "            new_word = logits.indices.numpy()[0, -1, sample]\n",
    "            tokens = tf.concat([tokens, [new_word]], axis=-1)\n",
    "        return tokenizer.detokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, dataset, epochs, loss_fn, optimizer):\n",
    "    for epoch in range(epochs):\n",
    "        for data in tqdm.tqdm(dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(data[0], True)\n",
    "                loss = loss_fn(data[1], predictions)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            loss = loss.numpy().mean()\n",
    "        print(f\"Epoch: {epoch+1}; Loss: {loss}\")\n",
    "        tf.print(model.generate_text(\"and god\", 20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:56<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Loss: 0.26514166593551636\n",
      "and god was set; saying unto be my father thou of my son esau hear so. he, he the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:57<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2; Loss: 0.24761104583740234\n",
      "and god will eat? bless him; because i will also and i be strohold unto my brother and after\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:59<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3; Loss: 0.17214134335517883\n",
      "and god will be blessed be blessed isaac his dead him that the earth; that was set in heth said to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:56<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4; Loss: 0.09850689768791199\n",
      "and god give his master an seed be put his death. after jacob that tow of rebekah but behold and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:52<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5; Loss: 0.1414528787136078\n",
      "and god and hoture sarai he dwelt by she took ishmael; thou unto me from him years in isaac was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:54<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6; Loss: 0.10460534691810608\n",
      "and god br raamah? son that jacob obey that he keditould des called ishmael before pharaoh rem righteous\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:54<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7; Loss: 0.1302645057439804\n",
      "and god divided see be as he dwelt by his younger abraham to a blessing esau in him as for them and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:53<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8; Loss: 0.09392839670181274\n",
      "and god will be blessed to abraham thee his son only thous sake nowstained his men take she said\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:55<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9; Loss: 0.07282131910324097\n",
      "and god of jacob sod esau upon abraham him; because isaac dwelt then digged before thou perfect. my curse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:49<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10; Loss: 0.097612164914608\n",
      "and god to witiver these things i of that day; his life because she was called flyso venison\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(num_layers=4, embedding_dim=100, num_heads=4, vocab_size=1500)\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "training_loop(model=model, dataset=dataset, epochs=10, loss_fn=loss_function, optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'god created sure sleep.. neither am this him forth bread and twentyion is from havilah in her and havilah when the angeleenservants me and hool i will i shall see withheldahamed that iting and k evil do by his brethren were covered the nakedness of bethel to hagar his house at beershebast the cityth year had pleaslled unto isaac they exceedingly down heteen unt jacob, but fat enar twelvey philistines looked took of'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_text(\"god created\", length=90, top_k=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5044b8152bc2b46737c38ed79d3b98afac6f664a9f6ca9801642503140b722e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
