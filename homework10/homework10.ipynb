{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import re\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first book of moses   called genesis       in the beginning god created the heaven and the earth       and the earth was without form  and void  and darkness was upon the face of the deep  and the spirit of god moved upon the face of the waters       and god said  let there be light  and there was light       and god saw the light  that it was good  and god divided the light from the darkness       and god called the light day  and the darkness he called night  and the evening and the morning were the first day       and god said  let there be a firmament in the midst of the waters  and let it divide the waters from the waters       and god made the firmament  and divided the waters which were under the firmament from the waters which were above the firmament  and it was so       and god called the firmament heaven  and the evening and the morning were the second day       and god said  let the waters under the heaven be gathered together unto one place  and let the dry land appear\n"
     ]
    }
   ],
   "source": [
    "with open(\"bible.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', ' ', text)\n",
    "    text = text[:10000] #smaller text for testing purposes\n",
    "    print(text[:1000])\n",
    "\n",
    "splitter = tf_text.WhitespaceTokenizer()\n",
    "tokens = splitter.split(text)\n",
    "tokens = list(tokens.numpy())\n",
    "vocabulary = {x: i for i, x in enumerate(np.unique(list(text)))}\n",
    "char_tokens = [vocabulary[char] for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 8], [18, 5], [8, 18], [8, 5], [8, 0], [5, 18], [5, 8], [5, 0], [5, 6], [0, 8]]\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 2\n",
    "sequences = []\n",
    "for i in range(0, len(char_tokens) - sequence_length):\n",
    "    for j in range(i-sequence_length, i+sequence_length+1):\n",
    "        if j == i or j < 0 or j >= len(char_tokens):\n",
    "            continue\n",
    "        sequences.append([char_tokens[i], char_tokens[j]])\n",
    "\n",
    "print(sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(sequences, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
    "    dataset = dataset.shuffle(len(sequences))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[16 19]\n",
      " [ 0  0]\n",
      " [ 0  0]\n",
      " [13  4]\n",
      " [ 0  0]\n",
      " [ 4  3]\n",
      " [16  8]\n",
      " [ 5 17]\n",
      " [21  0]\n",
      " [ 2  3]], shape=(10, 2), dtype=int32)\n",
      "<BatchDataset element_spec=TensorSpec(shape=(None, 2), dtype=tf.int32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "for target in generateDataset(sequences, 10).take(1):\n",
    "    print(target)\n",
    "\n",
    "print(generateDataset(sequences, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(tf.keras.Model) :\n",
    "  def __init__(self, vocabulary_size, embedding_size) :\n",
    "    super(SkipGram, self).__init__()\n",
    "    self.vocabulary_size = vocabulary_size\n",
    "    self.embedding_size = embedding_size\n",
    "    self.loss = tf.nn.nce_loss\n",
    "    self.build(\"\")\n",
    "\n",
    "  def build(self, empty) :\n",
    "      self.embeddings = self.add_weight(shape=(self.vocabulary_size, self.embedding_size), initializer='random_normal', trainable=True)\n",
    "      self.score_matrix = self.add_weight(shape=(self.vocabulary_size, self.embedding_size), initializer='random_normal', trainable=True)\n",
    "      self.nce_biases = tf.Variable(tf.zeros([self.vocabulary_size])) \n",
    "\n",
    "  def call(self, inputs) :\n",
    "    context = inputs[:, 0]\n",
    "    target = inputs[:, 1]\n",
    "    target = tf.expand_dims(target, axis=-1) #needed for nce_loss\n",
    "    inputs = tf.cast(inputs, tf.int64) #needed for sampler\n",
    "    target = tf.cast(target, tf.int64) #needed for sampler\n",
    "\n",
    "    embeddings = tf.nn.embedding_lookup(self.embeddings, context)\n",
    "  \n",
    "    #try to solve samplers range is too small error\n",
    "    samples = tf.random.log_uniform_candidate_sampler(true_classes=target, num_true=1, num_sampled=64, unique=True, range_max=self.vocabulary_size)\n",
    "    \n",
    "    loss = self.loss(labels=target, inputs=embeddings, weights=self.score_matrix, biases=self.nce_biases, num_sampled=64, num_classes=self.vocabulary_size, sampled_values=samples)\n",
    "    return tf.reduce_mean(loss)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(batch)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"skip_gram_11\" \"                 f\"(type SkipGram).\n\n{{function_node __wrapped__LogUniformCandidateSampler_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sampler's range is too small. [Op:LogUniformCandidateSampler]\n\nCall arguments received by layer \"skip_gram_11\" \"                 f\"(type SkipGram):\n  â€¢ inputs=tf.Tensor(shape=(100, 2), dtype=int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m      3\u001b[0m dataset \u001b[39m=\u001b[39m generateDataset(sequences, \u001b[39m100\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m train(model, dataset, optimizer, \u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, optimizer, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataset:\n\u001b[0;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m----> 5\u001b[0m         loss \u001b[39m=\u001b[39m model(batch)\n\u001b[0;32m      6\u001b[0m     gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, model\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, model\u001b[39m.\u001b[39mtrainable_variables))\n",
      "File \u001b[1;32mc:\\Users\\Moritz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[32], line 24\u001b[0m, in \u001b[0;36mSkipGram.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     21\u001b[0m embeddings \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39membedding_lookup(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings, context)\n\u001b[0;32m     23\u001b[0m \u001b[39m#try to solve samplers range is too small error\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m samples \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mlog_uniform_candidate_sampler(true_classes\u001b[39m=\u001b[39;49mtarget, num_true\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, num_sampled\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, unique\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, range_max\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocabulary_size)\n\u001b[0;32m     26\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(labels\u001b[39m=\u001b[39mtarget, inputs\u001b[39m=\u001b[39membeddings, weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore_matrix, biases\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnce_biases, num_sampled\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocabulary_size, sampled_values\u001b[39m=\u001b[39msamples)\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mreduce_mean(loss)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"skip_gram_11\" \"                 f\"(type SkipGram).\n\n{{function_node __wrapped__LogUniformCandidateSampler_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sampler's range is too small. [Op:LogUniformCandidateSampler]\n\nCall arguments received by layer \"skip_gram_11\" \"                 f\"(type SkipGram):\n  â€¢ inputs=tf.Tensor(shape=(100, 2), dtype=int32)"
     ]
    }
   ],
   "source": [
    "model = SkipGram(len(vocabulary), 10)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "dataset = generateDataset(sequences, 100)\n",
    "train(model, dataset, optimizer, 10)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no time to fix countless errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5044b8152bc2b46737c38ed79d3b98afac6f664a9f6ca9801642503140b722e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
